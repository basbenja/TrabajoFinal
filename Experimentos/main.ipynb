{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Importación de Librerías**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jaea6ppoSYFx"
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import json\n",
    "import numpy as np\n",
    "import mlflow\n",
    "import optuna\n",
    "import os\n",
    "import pprint\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from constants import *\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "from optuna.visualization import plot_pareto_front\n",
    "from optuna.study import MaxTrialsCallback\n",
    "from optuna.trial import TrialState\n",
    "\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "from utils.data_preprocessor import DataPreprocessor\n",
    "from utils.load_data import *\n",
    "from utils.train_predict import train_step, validate_step, validate_step_with_metrics, predict\n",
    "from utils.metrics import *\n",
    "from utils.optuna_utils import *\n",
    "from utils.mlflow_logger import MLflowLogger\n",
    "\n",
    "from models.instantiate import instantiate_model\n",
    "from models.lstm_v1 import *\n",
    "from models.lstm_v2 import *\n",
    "from models.gru import *\n",
    "from models.dense import *\n",
    "from models.fcn import *\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "np.random.seed(13)\n",
    "torch.manual_seed(13)\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"config.json\", \"r\") as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "pprint.pp(config)\n",
    "\n",
    "# PARTITIONS = config['partitions']\n",
    "GROUP = \"Grupo\" + config['group']\n",
    "SIMULATION = \"Simulacion\" + config['simulation']\n",
    "REQ_PERIODS = config['required_periods']\n",
    "METRICS = config['metrics']\n",
    "DIRECTIONS = config['directions']\n",
    "BETA = config['beta']\n",
    "MODEL_ARCH = config['model_arch']\n",
    "LOG_TO_MLFLOW = (config['log_to_mlflow'] == \"True\")\n",
    "\n",
    "TEMP_FEATS = [f'y(t-{i})' for i in range(REQ_PERIODS, 0, -1)]\n",
    "STAT_FEATS = ['inicio_prog']\n",
    "FEATS = STAT_FEATS + TEMP_FEATS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Log to MLflow: {LOG_TO_MLFLOW}\")\n",
    "mlflow_logger = MLflowLogger(\n",
    "    LOG_TO_MLFLOW,\n",
    "    TRACKING_SERVER_URI,\n",
    "    EXPERIMENT_NAME,\n",
    "    EXPERIMENT_TAGS\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GROUP_DIR = os.path.join(DATA_DIR, GROUP)\n",
    "GROUP_PARAMS_FILE = os.path.join(GROUP_DIR, f\"params_{GROUP}.json\")\n",
    "if os.path.exists(GROUP_PARAMS_FILE):\n",
    "    with open(GROUP_PARAMS_FILE, 'r') as f:\n",
    "        group_params = json.load(f)\n",
    "        mlflow_logger.log_params(group_params)\n",
    "else:\n",
    "    print(f\"Group params file not found: {GROUP_PARAMS_FILE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stata_filepath = os.path.join(GROUP_DIR, SIMULATION)\n",
    "if os.path.exists(stata_filepath):\n",
    "    df = pd.read_stata(stata_filepath)\n",
    "else:\n",
    "    print(f\"File {stata_filepath} not found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PlHQsGhFSYF1"
   },
   "outputs": [],
   "source": [
    "mlflow_logger.log_params({\n",
    "    \"group\": GROUP,\n",
    "    \"simulation\": SIMULATION,\n",
    "    \"filepath\": stata_filepath,\n",
    "    \"required_periods\": REQ_PERIODS\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Conjuntos de datos**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x5v_3f_FDqHc"
   },
   "source": [
    "**Terminología**:\n",
    "* Tipo 1: individuos tratados.\n",
    "* Tipo 2: individuos de control (i.e. podrían haber sido tratados pero por alguna razón no lo fueron)\n",
    "* Tipo 3: ni tratados ni de control."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **1. Separamos en tipo 1, tipo 2 y tipo 3.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 363
    },
    "id": "sjGpNBoTSYF2",
    "outputId": "a11baa73-7e16-4a5f-a30f-bb37905a19a7"
   },
   "outputs": [],
   "source": [
    "type1_df, type2_df, type3_df = get_dfs(df, REQ_PERIODS)\n",
    "\n",
    "# Chequeo para asegurarnos que los que tienen target 1 de tipo 2 se corresponden\n",
    "# con el inicio_prog de la base original\n",
    "assert(\n",
    "    all(\n",
    "        type2_df[type2_df['target'] == 1]['inicio_prog'] == \n",
    "        df[df['tipo'] == 2].groupby('id')['inicio_prog'].first()\n",
    "    )\n",
    ")\n",
    "\n",
    "# Cantidad de inicios de programa. Esto nos dice cuántos duplicados hay de cada\n",
    "# individuo de tipo 2y tipo 3.\n",
    "min_inicio_prog = type1_df['inicio_prog'].min()\n",
    "max_inicio_prog = type1_df['inicio_prog'].max()\n",
    "amount_inicio_prog = max_inicio_prog - min_inicio_prog + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **2. Separamos en train (que es sobre el que despues se va a hacer KFold) y test.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cantidad de individuos de cada tipo (con los duplicados)\n",
    "n1 = len(type1_df)\n",
    "n2 = len(type2_df)\n",
    "n3 = len(type3_df)\n",
    "\n",
    "# Cantidad total de individuos en el dataset POST-TRANSFORMACION. Estas son las\n",
    "# muestras que se van a usar en el proceso de entrenamiento y testing, por eso\n",
    "# se tienen en cuenta los duplicados.\n",
    "n = n1 + n2 + n3\n",
    "\n",
    "# Tamaño de los conjuntos de entrenamiento y testeo\n",
    "train_size = int(0.7 * n)\n",
    "test_size = n - train_size\n",
    "\n",
    "# En type2_df y type3_df, tenemos el mismo individuo pero con distinto inicio_prog\n",
    "# y por lo tanto distintos valores de y(t-1), y(t-2), ..., y(t-required_periods)\n",
    "# Lo que queremos es que todas las \"copias\" de un mismo individuo estén o todas\n",
    "# en train o todas en test. Por lo tanto, la selección no va a ser sobre filas\n",
    "# del dataframe sino sobre ids de individuos.\n",
    "type3_ids = type3_df.index.unique()\n",
    "\n",
    "# Todos los individuos de tipo 1 van al conjunto de entrenamiento, y los restantes\n",
    "# son de tipo 3 elegidos al azar\n",
    "# Dividimos por amount_inicio_prog porque vamos a seleccionar ids de individuos\n",
    "# de tipo 3, no filas\n",
    "# type3_train = int((train_size - n1) / amount_inicio_prog)\n",
    "n_type3_train = 1000\n",
    "\n",
    "# Ahora sí, seleccionamos aleatoriamente n3_train ids de individuos de tipo 3\n",
    "# para el conjunto de entrenamiento\n",
    "type3_train = np.random.choice(type3_ids, n_type3_train, replace=False)\n",
    "\n",
    "# Y nos quedamos con las filas del dataframe de tipo 3 que tienen los ids seleccionados\n",
    "type3_train_df = type3_df.loc[type3_train]\n",
    "\n",
    "# Los ids que no están en type3_train son para el conjunto de testeo\n",
    "n_type3_test = 500\n",
    "type3_test = list(set(type3_ids) - set(type3_train))\n",
    "type3_test = np.random.choice(type3_test, n_type3_test, replace=False)\n",
    "type3_test_df = type3_df.loc[type3_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.concat([type1_df, type3_train_df])\n",
    "X_train_df, y_train_df = train_df[FEATS], train_df['target']\n",
    "smote = SMOTE(sampling_strategy=0.6, k_neighbors=5, random_state=13)\n",
    "X_train_df, y_train_df = smote.fit_resample(X_train_df, y_train_df)\n",
    "\n",
    "test_df = pd.concat([type2_df, type3_test_df])\n",
    "X_test_df, y_test_df = test_df[FEATS], test_df['target']\n",
    "\n",
    "weights = compute_class_weight(\n",
    "    class_weight=\"balanced\", classes=np.unique(y_train_df), y=y_train_df\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_df.value_counts() / len(y_train_df) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_df.value_counts() / len(y_test_df) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **3.Estandarizamos ambos conjuntos en base a los datos de entrenamiento.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_preprocessor = DataPreprocessor(X_train_df, X_test_df, y_train_df, y_test_df)\n",
    "\n",
    "X_train_df_scaled, X_test_df_scaled = data_preprocessor.scale_data()\n",
    "X_train_df_scaled.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **4. Logueamos los datasets finales a MLFlow.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We build the datasets with the target column to load them into mlflow\n",
    "train_df = X_train_df_scaled.copy()\n",
    "train_df['target'] = y_train_df\n",
    "mlflow_logger.log_input(train_df, \"train\")\n",
    "\n",
    "test_df = X_test_df_scaled.copy()\n",
    "test_df['target'] = y_test_df\n",
    "mlflow_logger.log_input(test_df, \"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **5. Obtenemos estructura necesaria según la red que querramos usar y construimos Datasets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set, test_set = data_preprocessor.build_datasets(\n",
    "    MODEL_ARCH, TEMP_FEATS, STAT_FEATS, scale=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kptxCJGX-Swv"
   },
   "source": [
    "# **Búsqueda de hiperparámetros con Optuna**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **6. Dejamos seleccionado el constructor de modelo para la búsqueda de hiperparámetros**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "match MODEL_ARCH:\n",
    "    case \"lstm_v1\":\n",
    "        define_model = define_lstm_v1_model\n",
    "    case \"lstm_v2\":\n",
    "        define_model = define_lstm_v2_model\n",
    "    case \"gru\":\n",
    "        define_model = define_gru_model\n",
    "    case \"dense\":\n",
    "        define_model = define_dense_model\n",
    "    case \"fcn\":\n",
    "        define_model = define_fcn_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **7. Definimos los parámetros de la búsqueda de hiperparámetros**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamp = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "study_name = f\"study_{timestamp}\"\n",
    "study_n_trials = 30\n",
    "\n",
    "if len(METRICS) != len(DIRECTIONS):\n",
    "    raise ValueError(\"The number of metrics and directions should be the same\")\n",
    "\n",
    "mlflow_logger.log_params({\n",
    "    \"optuna_study_name\": study_name,\n",
    "    \"optuna_study_n_trials\": study_n_trials,\n",
    "    \"objective_metrics\": METRICS,\n",
    "    \"directions\": DIRECTIONS\n",
    "})\n",
    "\n",
    "if \"f_beta_score\" in METRICS:\n",
    "    mlflow_logger.log_param(\"f_beta_score\", BETA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "study = optuna.create_study(\n",
    "    direction=DIRECTIONS[0] if len(METRICS) == 1 else None,\n",
    "    directions=DIRECTIONS if len(METRICS) > 1 else None,\n",
    "    storage=OPTUNA_STORAGE,\n",
    "    study_name=study_name\n",
    ")\n",
    "study.set_metric_names(METRICS)\n",
    "\n",
    "study.optimize(\n",
    "    lambda trial: objective_cv(\n",
    "        trial, define_model, train_set, weights, METRICS, beta=BETA\n",
    "    ),\n",
    "    n_trials=20,\n",
    "    timeout=600,\n",
    "    n_jobs=-1,\n",
    "    callbacks=[MaxTrialsCallback(study_n_trials, states=(TrialState.COMPLETE,))],\n",
    "    show_progress_bar=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(METRICS) > 1:\n",
    "    print(f\"Number of trials on the Pareto front: {len(study.best_trials)}\")\n",
    "    for i, (metric, direction) in enumerate(zip(METRICS, DIRECTIONS)):\n",
    "        if direction == 'maximize':\n",
    "            best_trial = max(study.best_trials, key=lambda t: t.values[i])\n",
    "        elif direction == 'minimize':\n",
    "            best_trial = min(study.best_trials, key=lambda t: t.values[i])\n",
    "        print(f\"Metric: {metric}\")\n",
    "        print(f\"\\tDirection: {direction}\")\n",
    "        print(f\"\\tTrial number: {best_trial.number}\")\n",
    "        print(f\"\\tValues: {best_trial.values}\")\n",
    "        print(f\"\\tParams: {best_trial.params}\")\n",
    "    \n",
    "    fig = plot_pareto_front(study, target_names=METRICS)\n",
    "    mlflow_logger.log_plot(fig, \"pareto_front_plot.png\")\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_trials_info = get_best_trials_info(study, METRICS)\n",
    "best_trials_numbers = [trial['trial_number'] for trial in best_trials_info]\n",
    "pprint.pp(f\"Best trials info: {best_trials_info}\")\n",
    "pprint.pp(f\"Best trials numbers: {best_trials_numbers}\")\n",
    "\n",
    "mlflow_logger.log_json(best_trials_info, \"best_trials_info.json\")\n",
    "mlflow_logger.log_params({\n",
    "    \"best_trials_numbers\": best_trials_numbers\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(best_trials_numbers) == 1:\n",
    "    selected_trial = best_trials_numbers[0]\n",
    "else:\n",
    "    selected_trial = select_trial(best_trials_numbers)\n",
    "\n",
    "mlflow_logger.log_param(\"selected_trial_number\", selected_trial)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Entrenamiento del modelo con mejores hiperparámetros**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = study.trials[selected_trial].params\n",
    "\n",
    "model = instantiate_model(MODEL_ARCH, params)\n",
    "mlflow_logger.log_model_architecture(model)\n",
    "\n",
    "# Common parameters for all models\n",
    "epochs = params['n_epochs']\n",
    "\n",
    "# optimizer_name and lr parameters are for specifying the optimizer\n",
    "optimizer_name = params['optimizer']\n",
    "lr = params['lr']\n",
    "optimizer = getattr(optim, optimizer_name)(model.parameters(), lr=lr)\n",
    "\n",
    "# batch_size is for the training loop\n",
    "batch_size = params['batch_size']\n",
    "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True , num_workers=4)\n",
    "test_loader  = DataLoader(test_set , batch_size=batch_size, shuffle=False, num_workers=4)\n",
    "\n",
    "# loss function\n",
    "loss_fn = nn.BCEWithLogitsLoss(pos_weight=torch.tensor(weights[1], dtype=torch.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in tqdm(range(epochs)):\n",
    "    print(f\"Epoch {epoch} -----------------------------------------------------\")\n",
    "    avg_batch_loss_train = train_step(model, train_loader, loss_fn, optimizer)\n",
    "    print(f\"Average batch loss during training: {avg_batch_loss_train}\")\n",
    "\n",
    "    avg_batch_loss_test = validate_step(model, test_loader, loss_fn)\n",
    "    print(f\"Average batch loss during testing: {avg_batch_loss_test}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for epoch in tqdm(range(epochs)):\n",
    "#     print(f\"Epoch {epoch} -----------------------------------------------------\")\n",
    "#     avg_batch_loss_train = train_step(model, train_loader, loss_fn, optimizer)\n",
    "#     print(f\"Average batch loss during training: {avg_batch_loss_train}\")\n",
    "\n",
    "#     avg_batch_loss_test = validate_step(model, test_loader, loss_fn)\n",
    "#     print(f\"Average batch loss during testing: {avg_batch_loss_test}\")\n",
    "\n",
    "#     metrics_values = validate_step_with_metrics(\n",
    "#         model,\n",
    "#         X_test_tensor,\n",
    "#         y_test_tensor,\n",
    "#         loss_fn,\n",
    "#         METRICS,\n",
    "#         beta=BETA,\n",
    "#         train_features_mean=None\n",
    "#     )\n",
    "#     pprint.pp(metrics_values)\n",
    "#     mlflow_logger.log_metrics(metrics_values, step=epoch)\n",
    "\n",
    "# mlflow.pytorch.log_model(model, \"trained_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Evaluación del modelo entrenado**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tensor, y_train_tensor = train_set.tensors\n",
    "X_test_tensor, y_test_tensor = test_set.tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to('cpu')\n",
    "y_test_pred = model(X_test_tensor.to('cpu'))\n",
    "y_test_pred = predict(y_test_pred, loss_fn).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = confusion_matrix_plot(y_test_tensor, y_test_pred)\n",
    "mlflow_logger.log_plot(fig, \"confusion_matrix_plot.png\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_curve_plot(y_test_tensor, y_test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow_logger.end_run()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
