{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Importación de Librerías**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jaea6ppoSYFx"
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import json\n",
    "import numpy as np\n",
    "import optuna\n",
    "import os\n",
    "import pprint\n",
    "import time\n",
    "import torch\n",
    "torch.cuda.empty_cache()\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from constants import *\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from utils.data_preprocessor import DataPreprocessor\n",
    "from utils.load_data import *\n",
    "from utils.train_predict import train_step, validate_step, predict\n",
    "from utils.metrics import *\n",
    "from utils.optuna_utils import *\n",
    "from utils.mlflow_logger import MLflowLogger\n",
    "from utils.plots import *\n",
    "\n",
    "from models.instantiate import instantiate_model\n",
    "from models.lstm_v1 import *\n",
    "from models.lstm_v2 import *\n",
    "from models.gru import *\n",
    "from models.dense import *\n",
    "from models.conv import *\n",
    "from models.lstm_conv import *\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "np.random.seed(13)\n",
    "torch.manual_seed(13)\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"config.json\", \"r\") as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "pprint.pp(config)\n",
    "\n",
    "GROUP = \"Grupo\" + str(config['group'])\n",
    "SIMULATION = \"Simulacion\" + str(config['simulation'])\n",
    "REQ_PERIODS = config['required_periods']\n",
    "METRICS = config['metrics']\n",
    "DIRECTIONS = config['directions']\n",
    "BETA = config['beta']\n",
    "MODEL_ARCH = config['model_arch']\n",
    "LOG_TO_MLFLOW = (config['log_to_mlflow'] == \"True\")\n",
    "SCALE_DATA = (config['scale_data'] == \"True\")\n",
    "\n",
    "TEMP_FEATS = [f'y(t-{i})' for i in range(REQ_PERIODS, 0, -1)]\n",
    "STAT_FEATS = ['inicio_prog']\n",
    "FEATS = STAT_FEATS + TEMP_FEATS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Log to MLflow: {LOG_TO_MLFLOW}\")\n",
    "mlflow_logger = MLflowLogger(\n",
    "    LOG_TO_MLFLOW,\n",
    "    TRACKING_SERVER_URI,\n",
    "    EXPERIMENT_NAME + \"-\" + GROUP,\n",
    "    EXPERIMENT_TAGS\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GROUP_DIR = os.path.join(DATA_DIR, GROUP)\n",
    "GROUP_PARAMS_FILE = os.path.join(GROUP_DIR, f\"params_{GROUP}.json\")\n",
    "if os.path.exists(GROUP_PARAMS_FILE):\n",
    "    with open(GROUP_PARAMS_FILE, 'r') as f:\n",
    "        group_params = json.load(f)\n",
    "        mlflow_logger.log_param(\"data_params\", group_params)\n",
    "else:\n",
    "    print(f\"Group params file not found: {GROUP_PARAMS_FILE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stata_filepath = os.path.join(GROUP_DIR, SIMULATION + \".dta\")\n",
    "if os.path.exists(stata_filepath):\n",
    "    df = pd.read_stata(stata_filepath)\n",
    "else:\n",
    "    print(f\"File {stata_filepath} not found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PlHQsGhFSYF1"
   },
   "outputs": [],
   "source": [
    "mlflow_logger.log_params({\n",
    "    \"group\": GROUP,\n",
    "    \"simulation\": SIMULATION,\n",
    "    \"filepath\": stata_filepath,\n",
    "    \"required_periods\": REQ_PERIODS,\n",
    "    \"scale_data\": SCALE_DATA,\n",
    "    \"model_arch\": MODEL_ARCH,\n",
    "    \"metrics\": METRICS\n",
    "})\n",
    "\n",
    "if 'f_beta_score' in METRICS:\n",
    "    mlflow_logger.log_param(\"beta\", BETA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Conjuntos de datos**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x5v_3f_FDqHc"
   },
   "source": [
    "**Terminología**:\n",
    "* Tipo 1: individuos tratados.\n",
    "* Tipo 2: individuos de control (i.e. podrían haber sido tratados pero por alguna razón no lo fueron)\n",
    "* Tipo 3: ni tratados ni de control."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **1. Separamos en tipo 1, tipo 2 y tipo 3.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 363
    },
    "id": "sjGpNBoTSYF2",
    "outputId": "a11baa73-7e16-4a5f-a30f-bb37905a19a7"
   },
   "outputs": [],
   "source": [
    "type1_df, type2_df, type3_df = get_dfs(df, REQ_PERIODS)\n",
    "\n",
    "# Cantidad de inicios de programa. Esto nos dice cuántos duplicados hay de cada\n",
    "# individuo de tipo 2 y tipo 3.\n",
    "min_inicio_prog = type1_df['inicio_prog'].min()\n",
    "max_inicio_prog = type1_df['inicio_prog'].max()\n",
    "amount_inicio_prog = max_inicio_prog - min_inicio_prog + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **2. Separamos en train (que es sobre el que despues se va a hacer KFold) y test.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type1_ids = type1_df.index.unique()\n",
    "n_type1_train = 1000\n",
    "type1_train = np.random.choice(type1_ids, n_type1_train, replace=False)\n",
    "type1_train_df = type1_df.loc[type1_train]\n",
    "\n",
    "type3_ids = type3_df.index.unique()\n",
    "n_type3_train = 1000\n",
    "type3_train = np.random.choice(type3_ids, n_type3_train, replace=False)\n",
    "type3_train_df = type3_df.loc[type3_train]\n",
    "\n",
    "# Los ids que no están en type3_train son para e\n",
    "# l conjunto de testeo\n",
    "n_type3_test = 2500\n",
    "type3_test = list(set(type3_ids) - set(type3_train))\n",
    "type3_test = np.random.choice(type3_test, n_type3_test, replace=False)\n",
    "type3_test_df = type3_df.loc[type3_test]\n",
    "\n",
    "for name, ids in [(\"ninis_ids_train\", type3_train), (\"ninis_ids_test\", type3_test)]:\n",
    "    mlflow_logger.log_json({name: ids.tolist(), \"amount\": len(ids)}, f\"{name}.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.concat([type1_train_df, type3_train_df])\n",
    "X_train_df, y_train_df = train_df[FEATS], train_df['target']\n",
    "\n",
    "test_df = pd.concat([type2_df, type3_test_df])\n",
    "X_test_df, y_test_df = test_df[FEATS], test_df['target']\n",
    "\n",
    "weights = compute_class_weight(\n",
    "    class_weight=\"balanced\", classes=np.unique(y_train_df), y=y_train_df\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_proportions = y_train_df.value_counts(normalize=True).to_dict()\n",
    "train_proportions = {k: round(v*100, 2) for k, v in train_proportions.items()}\n",
    "print(f\"Train proportions: {train_proportions}\")\n",
    "\n",
    "test_proportions  = y_test_df.value_counts(normalize=True).to_dict()\n",
    "test_proportions = {k: round(v*100, 2) for k, v in test_proportions.items()}\n",
    "print(f\"Test proportions:  {test_proportions}\")\n",
    "\n",
    "mlflow_logger.log_params({\n",
    "    \"train_proportions\": train_proportions,\n",
    "    \"test_proportions\": test_proportions\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **3.Estandarizamos ambos conjuntos en base a los datos de entrenamiento.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las columnas de nuestro dataset corresponden a distintos pasos de tiempo para \n",
    "cada individuo. Si estandarizamos a lo largo de cada columna, perdemos la relación\n",
    "que hay entre los valores de una misma serie de tiempo, que es lo que nos interesa\n",
    "en este caso (que la red identifique la tendencia creciente).\n",
    "\n",
    "Lo que vamos a probar es escalar a lo largo de las filas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_preprocessor = DataPreprocessor(STAT_FEATS, TEMP_FEATS)\n",
    "\n",
    "if SCALE_DATA:\n",
    "    X_train_df_scaled, X_test_df_scaled = data_preprocessor._scale_data(X_train_df, X_test_df)\n",
    "    # X_train_temp_flattened = X_train_df[TEMP_FEATS].values.flatten()\n",
    "\n",
    "    # X_train_temp_mean = np.mean(X_train_temp_flattened)\n",
    "    # X_train_temp_std  = np.std(X_train_temp_flattened)\n",
    "\n",
    "    # X_train_df_scaled[TEMP_FEATS] = (X_train_df[TEMP_FEATS] - X_train_temp_mean) / X_train_temp_std\n",
    "    # X_test_df_scaled[TEMP_FEATS]  = (X_test_df[TEMP_FEATS] - X_train_temp_mean) / X_train_temp_std\n",
    "else:\n",
    "    X_train_df_scaled, X_test_df_scaled = X_train_df, X_test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "treated_df_scaled = X_train_df_scaled[y_train_df==1]\n",
    "control_df_scaled = X_test_df_scaled[y_test_df==1]\n",
    "nini_df_scaled = X_train_df_scaled[y_train_df==0]\n",
    "\n",
    "for df, label in [\n",
    "    (treated_df_scaled, \"Tratados\"), (control_df_scaled, \"de Control\"), (nini_df_scaled, \"Ninis\")\n",
    "]:\n",
    "    fig, ax = plot_time_series(df[TEMP_FEATS], 15, label)\n",
    "    # fig.show()\n",
    "    mlflow_logger.log_plot(fig, f\"plot_time_series_{label}.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **4. Logueamos los datasets finales a MLFlow.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We build the datasets with the target column to load them into mlflow\n",
    "train_df = X_train_df_scaled.copy()\n",
    "train_df['target'] = y_train_df\n",
    "mlflow_logger.log_input(train_df, \"train\")\n",
    "\n",
    "test_df = X_test_df_scaled.copy()\n",
    "test_df['target'] = y_test_df\n",
    "mlflow_logger.log_input(test_df, \"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **5. Obtenemos estructura necesaria según la red que querramos usar y construimos Datasets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set, test_set = data_preprocessor.build_datasets(\n",
    "    X_train_df_scaled, X_test_df_scaled, y_train_df, y_test_df, MODEL_ARCH\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kptxCJGX-Swv"
   },
   "source": [
    "# **Búsqueda de hiperparámetros con Optuna**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **6. Dejamos seleccionado el constructor de modelo para la búsqueda de hiperparámetros**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "match MODEL_ARCH:\n",
    "    case \"lstm_v1\":\n",
    "        define_model = define_lstm_v1_model\n",
    "        input_size = 2\n",
    "    case \"lstm_v2\":\n",
    "        define_model = define_lstm_v2_model\n",
    "        input_size = 1\n",
    "    case \"gru\":\n",
    "        define_model = define_gru_model\n",
    "        input_size = 2\n",
    "    case \"dense\":\n",
    "        define_model = define_dense_model\n",
    "        input_size = len(FEATS)\n",
    "    case \"conv\":\n",
    "        define_model = define_conv_model\n",
    "        input_size = 1\n",
    "    case \"lstm_conv\":\n",
    "        define_model = define_lstm_conv_model\n",
    "        input_size = 1\n",
    "print(f\"Selected model: {MODEL_ARCH}, Input size: {input_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **7. Definimos los parámetros de la búsqueda de hiperparámetros**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamp = time.strftime(\"%d%m%Y-%H%M%S\")\n",
    "study_name = f\"study_{timestamp}\"\n",
    "\n",
    "if len(METRICS) != len(DIRECTIONS):\n",
    "    raise ValueError(\"The number of metrics and directions should be the same\")\n",
    "\n",
    "optuna_params = {\n",
    "    \"optuna_study_name\": study_name,\n",
    "    \"objective_metrics\": METRICS,\n",
    "    \"directions\": DIRECTIONS\n",
    "}\n",
    "\n",
    "if \"f_beta_score\" in METRICS:\n",
    "    optuna_params[\"beta\"] = BETA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "study = optuna.create_study(\n",
    "    direction=DIRECTIONS[0] if len(METRICS) == 1 else None,\n",
    "    directions=DIRECTIONS if len(METRICS) > 1 else None,\n",
    "    storage=OPTUNA_STORAGE,\n",
    "    study_name=study_name,\n",
    "    load_if_exists=True\n",
    ")\n",
    "study.set_metric_names(METRICS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_worker(\n",
    "    n_trials, study_name, optuna_storage, define_model, input_size,\n",
    "    train_set, loss_fn, metrics, beta\n",
    "):\n",
    "    study = optuna.load_study(\n",
    "        study_name=study_name,\n",
    "        storage=optuna_storage\n",
    "    )\n",
    "    study.optimize(\n",
    "        lambda trial: objective_cv(\n",
    "            trial, define_model, input_size, train_set, loss_fn, metrics, beta=beta\n",
    "        ),\n",
    "        n_trials=n_trials,\n",
    "        n_jobs=-1,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.BCEWithLogitsLoss(pos_weight=torch.tensor(weights[1], dtype=torch.float32))\n",
    "print(f\"Starting hyperparameter optimization with {N_PROCESSES} processes\")\n",
    "Parallel(n_jobs=N_PROCESSES)(\n",
    "    delayed(run_worker)(\n",
    "        TRIALS_PER_PROCESS, study_name, OPTUNA_STORAGE, define_model, input_size, train_set,\n",
    "        loss_fn, METRICS, BETA\n",
    "    )\n",
    "    for _ in range(N_PROCESSES)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(METRICS) > 1:\n",
    "    fig = pareto_front(study, METRICS, DIRECTIONS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_trials_info = get_best_trials_info(study, METRICS)\n",
    "best_trials_numbers = [trial['trial_number'] for trial in best_trials_info]\n",
    "pprint.pp(f\"Best trials info: {best_trials_info}\")\n",
    "pprint.pp(f\"Best trials numbers: {best_trials_numbers}\")\n",
    "\n",
    "mlflow_logger.log_json(best_trials_info, \"best_trials_info.json\")\n",
    "optuna_params[\"best_trials_numbers\"] = best_trials_numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(best_trials_numbers) == 1:\n",
    "    selected_trial = best_trials_numbers[0]\n",
    "else:\n",
    "    # Desempatamos por hidden_size\n",
    "    min_hidden_size = min(best_trials_info, key=lambda x: x['params']['hidden_size'])\n",
    "    selected_trial = min_hidden_size['trial_number']\n",
    "\n",
    "optuna_params[\"selected_trial_number\"] = selected_trial\n",
    "optuna_params[\"metric_best_value\"] = study.trials[selected_trial].value\n",
    "mlflow_logger.log_param(\"optuna_params\", optuna_params)\n",
    "\n",
    "mlflow_logger.log_param(\"metric_best_value_in_optimization\", optuna_params[\"metric_best_value\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Entrenamiento del modelo con mejores hiperparámetros**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = study.trials[selected_trial].params\n",
    "params['num_layers'] = N_LAYERS\n",
    "params['n_epochs'] = N_EPOCHS\n",
    "params['optimizer'] = OPTIMIZER\n",
    "params['model_name'] = MODEL_ARCH\n",
    "mlflow_logger.log_param(\"train_params\", params)\n",
    "\n",
    "model = instantiate_model(MODEL_ARCH, input_size, params).to(device)\n",
    "mlflow_logger.log_model_architecture(model)\n",
    "\n",
    "# optimizer_name and lr parameters are for specifying the optimizer\n",
    "lr = params['lr']\n",
    "optimizer = getattr(optim, OPTIMIZER)(model.parameters(), lr=lr)\n",
    "\n",
    "# batch_size is for the training loop\n",
    "batch_size = params['batch_size']\n",
    "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True , num_workers=8)\n",
    "test_loader  = DataLoader(test_set , batch_size=batch_size, shuffle=False, num_workers=8)\n",
    "\n",
    "# loss function\n",
    "loss_fn = nn.BCEWithLogitsLoss(pos_weight=torch.tensor(weights[1], dtype=torch.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_losses_train = []\n",
    "epoch_losses_test  = []\n",
    "epoch_metrics_test = {metric: [] for metric in METRICS}\n",
    "\n",
    "print(\"Training model with best hyperparameters\")\n",
    "for epoch in tqdm(range(N_EPOCHS)):\n",
    "    print(f\"Epoch {epoch} -----------------------------------------------------\")\n",
    "    # This value is just for monitoring\n",
    "    avg_loss_trainining = train_step(model, train_loader, loss_fn, optimizer)\n",
    "    print(f\"  Average loss in train set during epoch: {avg_loss_trainining:.6f}\")\n",
    "\n",
    "    avg_loss_train, _ = validate_step(\n",
    "        model, train_loader, loss_fn, METRICS, beta=BETA\n",
    "    )\n",
    "    print(f\"  Average loss in train set: {avg_loss_train:.6f}\")\n",
    "    epoch_losses_train.append(avg_loss_train)\n",
    "\n",
    "    avg_loss_test, metrics_test = validate_step(\n",
    "        model, test_loader, loss_fn, METRICS, beta=BETA\n",
    "    )\n",
    "    print(f\"  Average loss in test set: {avg_loss_test:.6f}\")\n",
    "    epoch_losses_test.append(avg_loss_test)\n",
    "    print(f\"  Metrics in test set:\")\n",
    "    for metric, value in metrics_test.items():\n",
    "        print(f\"    - {metric}: {value:.6f}\")\n",
    "        epoch_metrics_test[metric].append(value)\n",
    "print(\"Training finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for metric in METRICS:\n",
    "    mlflow_logger.log_param(f\"{metric}_test_after_training\", epoch_metrics_test[metric][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "*input_batch, _ = next(iter(train_loader))\n",
    "\n",
    "if len(input_batch) == 1:\n",
    "    example_input = input_batch[0].cpu().numpy()\n",
    "else:\n",
    "    example_input = {\n",
    "        \"temporal\": input_batch[0].cpu().numpy(),\n",
    "        \"static\": input_batch[1].cpu().numpy()\n",
    "    }\n",
    "\n",
    "mlflow_logger.log_model(model.to('cpu'), \"trained_model\", example_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = epoch_vs_loss_plot(epoch_losses_train, epoch_losses_test)\n",
    "fig.show()\n",
    "\n",
    "mlflow_logger.log_plot(fig, \"epoch_vs_loss.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for metric in METRICS:\n",
    "    if metric == 'f_beta_score':\n",
    "        metric_name = f\"Puntaje $F_{{{BETA}}}$\"\n",
    "    elif metric == 'f1_score':\n",
    "        metric_name = \"Puntaje $F_1$\"\n",
    "    else:\n",
    "        metric_name = metric.capitalize()\n",
    "    fig, ax = epoch_vs_metric_plot(metric_name, epoch_metrics_test[metric])\n",
    "    fig.show()\n",
    "\n",
    "    mlflow_logger.log_plot(fig, f\"epoch_vs_{metric}.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Evaluación del modelo entrenado**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "model.to('cpu')\n",
    "\n",
    "try:\n",
    "    X_test_tensor, y_test_tensor = test_set.tensors\n",
    "    y_test_pred = model(X_test_tensor.to('cpu'))\n",
    "except AttributeError:\n",
    "    X_test_temporal_tensor, X_test_static_tensor, y_test_tensor = (\n",
    "        test_set.temporal_data, test_set.static_data, test_set.labels\n",
    "    )\n",
    "    y_test_pred = model(X_test_temporal_tensor.to('cpu'), X_test_static_tensor.to('cpu'))\n",
    "\n",
    "y_test_pred = predict(y_test_pred, loss_fn).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = confusion_matrix_plot(y_test_tensor, y_test_pred)\n",
    "fig.show()\n",
    "\n",
    "mlflow_logger.log_plot(fig, \"confusion_matrix_plot.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report_str = classification_report(y_test_tensor, y_test_pred)\n",
    "print(report_str)\n",
    "\n",
    "mlflow_logger.log_json(\n",
    "    classification_report(y_test_tensor, y_test_pred, output_dict=True),\n",
    "    \"classification_report.json\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = roc_curve_plot(y_test_tensor, y_test_pred)\n",
    "fig.show()\n",
    "\n",
    "mlflow_logger.log_plot(fig, \"roc_curve_plot.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow_logger.end_run()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "tf-3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
