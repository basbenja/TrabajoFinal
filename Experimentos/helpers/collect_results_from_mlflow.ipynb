{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8bbfb01",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import mlflow\n",
    "\n",
    "from collections import defaultdict\n",
    "from constants import TRACKING_SERVER_URI\n",
    "\n",
    "mlflow.set_tracking_uri(TRACKING_SERVER_URI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4e21c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_name = \"TF-Grupo1-CompML\"\n",
    "experiment = mlflow.get_experiment_by_name(experiment_name)\n",
    "\n",
    "runs_list = mlflow.search_runs(\n",
    "    experiment_ids=[experiment.experiment_id],\n",
    "    output_format=\"list\"\n",
    ")\n",
    "\n",
    "hyperparams_runs = {'hidden_size': [], 'dropout': [], 'lr': [], 'batch_size': []}\n",
    "for run in runs_list:\n",
    "    run_data = run.data\n",
    "    run_info = run.info\n",
    "    status = run_info.status\n",
    "    if status != \"FINISHED\":\n",
    "        continue\n",
    "\n",
    "    run_params = run_data.params    # type: dict\n",
    "\n",
    "    train_params = run_params['train_params'].replace(\"'\", '\"')\n",
    "    train_params = json.loads(train_params)\n",
    "\n",
    "    for k, v in train_params.items():\n",
    "        if k in hyperparams_runs:\n",
    "            hyperparams_runs[k].append(v)\n",
    "\n",
    "import pandas as pd\n",
    "hyperparams_df = pd.DataFrame(hyperparams_runs)\n",
    "hyperparams_df.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adfbaaa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for hyperparam in hyperparams_df.columns:\n",
    "    print(hyperparams_df[hyperparam].value_counts() / len(hyperparams_df) * 100)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ede7fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_name = \"TF-Grupo3-CompML\"\n",
    "experiment = mlflow.get_experiment_by_name(experiment_name)\n",
    "\n",
    "runs_list = mlflow.search_runs(\n",
    "    experiment_ids=[experiment.experiment_id],\n",
    "    output_format=\"list\"\n",
    ")\n",
    "\n",
    "date_runs = defaultdict(list)\n",
    "for run in runs_list:\n",
    "    run_data = run.data\n",
    "    run_info = run.info\n",
    "\n",
    "    run_params = run_data.params    # type: dict\n",
    "    run_model_arch = run_params['model_arch']\n",
    "    if run_model_arch != \"conv\":\n",
    "        continue\n",
    "\n",
    "    run_status = run_info.status\n",
    "    if run_status != \"FINISHED\":\n",
    "        continue\n",
    "\n",
    "    run_name = run_info.run_name\n",
    "    run_date = run_name.split(\"_\")[1].split(\"-\")[0]\n",
    "    date_runs[run_date].append(run_params['f1_score_test_after_training'])\n",
    "\n",
    "date_runs_today = date_runs['20250529']\n",
    "date_runs_before = date_runs['20250527'] + date_runs['20250528']\n",
    "print(f\"Runs today: {len(date_runs_today)}\")\n",
    "print(f\"Runs before today: {len(date_runs_before)}\")\n",
    "\n",
    "avg_today = sum(float(x) for x in date_runs_today) / len(date_runs_today)\n",
    "avg_before = sum(float(x) for x in date_runs_before) / len(date_runs_before)\n",
    "print(f\"Average F1 score today: {avg_today:.4f}\")\n",
    "print(f\"Average F1 score before today: {avg_before:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
