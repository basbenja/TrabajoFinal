{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b110f05e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dfa631e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import mlflow\n",
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "import pandas as pd\n",
    "import pprint as pp\n",
    "import re\n",
    "\n",
    "import sys\n",
    "sys.path.append(os.path.dirname(os.getcwd()))\n",
    "\n",
    "from constants import TRACKING_SERVER_URI, DATA_DIR, EXPERIMENT_PREFIX\n",
    "from utils.plots import confusion_matrix_plot\n",
    "from utils.load_data import get_dfs\n",
    "from utils.data_preprocessor import DataPreprocessor\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "mlflow.set_tracking_uri(TRACKING_SERVER_URI)\n",
    "\n",
    "np.random.seed(13)\n",
    "torch.manual_seed(13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1027afa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"MLFLOW_ENABLE_ARTIFACTS_PROGRESS_BAR\"] = \"False\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "006cc03e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../config.json\", \"r\") as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "GROUP = \"Grupo\" + str(config['group'])\n",
    "MODEL_ARCH = config['model_arch']\n",
    "COMPARISON = config['comparison']\n",
    "\n",
    "print(f\"{GROUP} - {MODEL_ARCH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d61efd3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "GROUP_DIR = os.path.join(DATA_DIR, GROUP)\n",
    "GROUP_PARAMS_FILE = os.path.join(GROUP_DIR, f\"params_{GROUP}.json\")\n",
    "if os.path.exists(GROUP_PARAMS_FILE):\n",
    "    with open(GROUP_PARAMS_FILE, 'r') as f:\n",
    "        group_params = json.load(f)\n",
    "else:\n",
    "    print(f\"Group params file not found: {GROUP_PARAMS_FILE}\")\n",
    "\n",
    "REQ_PERIODS = group_params['first_tr_period'] - 1\n",
    "TEMP_FEATS = [f'y(t-{i})' for i in range(REQ_PERIODS, 0, -1)]\n",
    "STAT_FEATS = ['inicio_prog']\n",
    "FEATS = STAT_FEATS + TEMP_FEATS\n",
    "\n",
    "N_PER_DEP = group_params['n_per_dep']\n",
    "\n",
    "print(f\"Períodos observados:     {REQ_PERIODS}\")\n",
    "print(f\"Períodos de dependencia: {N_PER_DEP}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f49972d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_threshold_for_f1(y_test_true, y_test_pred_prob):\n",
    "    thresholds = np.linspace(0, 1, 101)\n",
    "    f1_scores = []\n",
    "\n",
    "    for thresh in thresholds:\n",
    "        y_test_pred_class = (y_test_pred_prob >= thresh).astype(int)\n",
    "        f1 = f1_score(y_test_true, y_test_pred_class)\n",
    "        f1_scores.append(f1)\n",
    "\n",
    "    best_idx = np.argmax(f1_scores)\n",
    "    best_thresh = thresholds[best_idx]\n",
    "    best_f1 = f1_scores[best_idx]\n",
    "\n",
    "    return best_thresh, best_f1\n",
    "\n",
    "\n",
    "def evaluate_model(model, test_set):\n",
    "    test_loader = DataLoader(test_set, batch_size=64, shuffle=False)\n",
    "\n",
    "    y_test_pred = []\n",
    "    y_test_true = []\n",
    "\n",
    "    model.eval()\n",
    "    model.to('cpu')\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            try:\n",
    "                X, y = batch\n",
    "                X = X.to('cpu')\n",
    "                logits = model(X)\n",
    "            except ValueError:\n",
    "                X_temporal, X_static, y = batch\n",
    "                X_temporal, X_static = X_temporal.to('cpu'), X_static.to('cpu')\n",
    "                logits = model(X_temporal, X_static)\n",
    "\n",
    "            y_test_true.extend(y.squeeze().cpu().tolist())\n",
    "            y_test_pred.extend(logits.squeeze().cpu().tolist())\n",
    "\n",
    "    return y_test_true, y_test_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9667af58",
   "metadata": {},
   "outputs": [],
   "source": [
    "MLFLOW_STORAGE_PATH = \"/home/basbenja/Facultad/TrabajoFinal/mountpoint_mlflow_storage\"\n",
    "\n",
    "ARTIFACTS_PATH = os.path.join(MLFLOW_STORAGE_PATH, \"mlartifacts\")\n",
    "RUNS_PATH = os.path.join(MLFLOW_STORAGE_PATH, \"mlruns\")\n",
    "\n",
    "EXPERIMENT_NAME = f\"{EXPERIMENT_PREFIX}-{GROUP}-Comp{COMPARISON}\"\n",
    "\n",
    "experiment = mlflow.get_experiment_by_name(EXPERIMENT_NAME)\n",
    "experiment_id = experiment.experiment_id\n",
    "\n",
    "experiment_artifacts_path = os.path.join(ARTIFACTS_PATH, experiment_id)\n",
    "experiment_runs_path      = os.path.join(RUNS_PATH, experiment_id)\n",
    "\n",
    "runs = mlflow.search_runs(\n",
    "    experiment_ids=[experiment_id],\n",
    "    output_format=\"list\",\n",
    "    filter_string=f\"params.model_arch = '{MODEL_ARCH}'\",\n",
    ")\n",
    "\n",
    "print(f\"{EXPERIMENT_NAME}: ID {experiment_id}\")\n",
    "print(f\"Cantidad de runs: {len(runs)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c3f21d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, run in enumerate(runs):\n",
    "    run_info = run.info\n",
    "    run_id = run_info.run_id\n",
    "    run_name = run_info.run_name\n",
    "    run_params = run.data.params\n",
    "    run_artifact_uri = run.info.artifact_uri\n",
    "\n",
    "    print(f\"Run {i+1} / {len(runs)}: {run_name} - ID {run_id}\")\n",
    "\n",
    "    run_artifacts_path = os.path.join(experiment_artifacts_path, run_id, 'artifacts')\n",
    "    run_params_path    = os.path.join(experiment_runs_path     , run_id, 'params')\n",
    "\n",
    "    print(\"    Loading model\")\n",
    "    model_uri = f\"runs:/{run_id}/trained_model\"\n",
    "    model = mlflow.pytorch.load_model(model_uri)\n",
    "\n",
    "    simulation = run_params['simulation']\n",
    "    stata_filepath = os.path.join(GROUP_DIR, simulation + \".dta\")\n",
    "    df = pd.read_stata(stata_filepath)\n",
    "\n",
    "    type3_train_ids_logged = mlflow.artifacts.load_dict(run_artifact_uri + \"/ninis_ids_train.json\")['ninis_ids_train']\n",
    "    type3_test_ids_logged  = mlflow.artifacts.load_dict(run_artifact_uri + \"/ninis_ids_test.json\" )['ninis_ids_test']\n",
    "\n",
    "    treated_orig = df[df.tratado == 1]\n",
    "    control_orig = df[df.control == 1]\n",
    "    nini_train_orig = df[df.id.isin(type3_train_ids_logged)]\n",
    "    nini_test_orig  = df[df.id.isin(type3_test_ids_logged)]\n",
    "\n",
    "    tr_starts = df.inicio_prog.unique()[df.inicio_prog.unique() > 0]\n",
    "\n",
    "    cohorts_avg_f1 = 0\n",
    "    cohort_avg_precision = 0\n",
    "    cohort_avg_recall = 0\n",
    "\n",
    "    for tr_start in tr_starts:\n",
    "        print(f\"    Inicio de programa {tr_start}\")\n",
    "\n",
    "        treated_in_cohort = treated_orig[treated_orig.inicio_prog == tr_start]\n",
    "        control_in_cohort = control_orig[control_orig.inicio_prog == tr_start]\n",
    "\n",
    "        df_cohort = pd.concat([\n",
    "            treated_in_cohort,\n",
    "            control_in_cohort,\n",
    "            nini_train_orig,\n",
    "            nini_test_orig,\n",
    "        ])\n",
    "\n",
    "        type1_df, type2_df, type3_df = get_dfs(df_cohort, REQ_PERIODS)\n",
    "\n",
    "        type3_train_df = type3_df.loc[type3_train_ids_logged]\n",
    "        type3_test_df  = type3_df.loc[type3_test_ids_logged]\n",
    "\n",
    "        # Esto no se usa para nada, solo para poder llamar a la función\n",
    "        # `build_datasets` del DataPreprocessor\n",
    "        train_df = pd.concat([type1_df, type3_train_df])\n",
    "        X_train_df, y_train_df = train_df[FEATS], train_df['target']\n",
    "\n",
    "        test_df = pd.concat([type2_df, type3_test_df])\n",
    "        X_test_df, y_test_df = test_df[FEATS], test_df['target']\n",
    "\n",
    "        data_preprocessor = DataPreprocessor(STAT_FEATS, TEMP_FEATS)\n",
    "        train_set, test_set = data_preprocessor.build_datasets(\n",
    "            X_train_df, X_test_df, y_train_df, y_test_df, MODEL_ARCH\n",
    "        )\n",
    "\n",
    "        print(\"    Evaluating model\")\n",
    "        y_test_true, y_test_pred = evaluate_model(model, test_set)\n",
    "\n",
    "        y_test_pred = np.array(y_test_pred)\n",
    "        y_test_pred_prob = 1 / (1 + np.exp(-y_test_pred))\n",
    "        y_test_pred_class = (y_test_pred_prob >= 0.5).astype(int)\n",
    "\n",
    "        inicio_prog_dir_path = os.path.join(run_artifacts_path, f\"inicio_prog_{tr_start}\")\n",
    "        os.makedirs(inicio_prog_dir_path, exist_ok=True)\n",
    "\n",
    "        report_dict = classification_report(y_test_true, y_test_pred_class, output_dict=True)\n",
    "        report_path = os.path.join(inicio_prog_dir_path, \"classification_report.json\")\n",
    "        with open(report_path, 'w') as f:\n",
    "            json.dump(report_dict, f)\n",
    "\n",
    "        fig, ax, confusion_dict = confusion_matrix_plot(y_test_true, y_test_pred_class)\n",
    "        conf_matrix_path = os.path.join(inicio_prog_dir_path, \"confusion_matrix_plot.png\")\n",
    "        fig.savefig(conf_matrix_path)\n",
    "        plt.close(fig)\n",
    "\n",
    "        confusion_dict_path = os.path.join(inicio_prog_dir_path, \"confusion_dict.json\")\n",
    "        with open(confusion_dict_path, 'w') as f:\n",
    "            json.dump(confusion_dict, f)\n",
    "\n",
    "        cohorts_avg_f1 += report_dict['1.0']['f1-score']\n",
    "        cohort_avg_precision += report_dict['1.0']['precision']\n",
    "        cohort_avg_recall += report_dict['1.0']['recall']\n",
    "\n",
    "    cohorts_avg_f1 /= len(tr_starts)\n",
    "    cohort_avg_precision /= len(tr_starts)\n",
    "    cohort_avg_recall /= len(tr_starts)\n",
    "\n",
    "    for k, v in {\n",
    "        \"cohorts_avg_f1\": cohorts_avg_f1,\n",
    "        \"cohort_avg_precision\": cohort_avg_precision,\n",
    "        \"cohort_avg_recall\": cohort_avg_recall\n",
    "    }.items():\n",
    "        metric_path = os.path.join(run_params_path, k)\n",
    "        with open(metric_path, 'w') as f:\n",
    "            f.write(str(v))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
