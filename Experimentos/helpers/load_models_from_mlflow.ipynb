{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b110f05e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dfa631e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import mlflow\n",
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "import sys\n",
    "sys.path.append(os.path.dirname(os.getcwd()))\n",
    "\n",
    "from constants import TRACKING_SERVER_URI, DATA_DIR, EXPERIMENT_PREFIX\n",
    "from utils.plots import roc_curve_plot\n",
    "from utils.load_data import get_dfs\n",
    "from utils.data_preprocessor import DataPreprocessor\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "mlflow.set_tracking_uri(TRACKING_SERVER_URI)\n",
    "\n",
    "np.random.seed(13)\n",
    "torch.manual_seed(13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1027afa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"MLFLOW_ENABLE_ARTIFACTS_PROGRESS_BAR\"] = \"False\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "006cc03e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"config.json\", \"r\") as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "GROUP = \"Grupo\" + str(config['group'])\n",
    "MODEL_ARCH = config['model_arch']\n",
    "COMPARISON = config['comparison']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d61efd3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "GROUP_DIR = os.path.join(DATA_DIR, GROUP)\n",
    "GROUP_PARAMS_FILE = os.path.join(GROUP_DIR, f\"params_{GROUP}.json\")\n",
    "if os.path.exists(GROUP_PARAMS_FILE):\n",
    "    with open(GROUP_PARAMS_FILE, 'r') as f:\n",
    "        group_params = json.load(f)\n",
    "else:\n",
    "    print(f\"Group params file not found: {GROUP_PARAMS_FILE}\")\n",
    "\n",
    "REQ_PERIODS = group_params['first_tr_period'] - 1\n",
    "TEMP_FEATS = [f'y(t-{i})' for i in range(REQ_PERIODS, 0, -1)]\n",
    "STAT_FEATS = ['inicio_prog']\n",
    "FEATS = STAT_FEATS + TEMP_FEATS\n",
    "\n",
    "N_PER_DEP = group_params['n_per_dep']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f49972d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_threshold_for_f1(y_test_true, y_test_pred_prob):\n",
    "    # Search for best threshold\n",
    "    thresholds = np.linspace(0, 1, 101)  # 0.00, 0.01, ..., 1.00\n",
    "    f1_scores = []\n",
    "\n",
    "    for thresh in thresholds:\n",
    "        y_test_pred_class = (y_test_pred_prob >= thresh).astype(int)\n",
    "        f1 = f1_score(y_test_true, y_test_pred_class)\n",
    "        f1_scores.append(f1)\n",
    "\n",
    "    # Get the best threshold\n",
    "    best_idx = np.argmax(f1_scores)\n",
    "    best_thresh = thresholds[best_idx]\n",
    "    best_f1 = f1_scores[best_idx]\n",
    "\n",
    "    return best_thresh, best_f1\n",
    "\n",
    "def evaluate_model(model, test_set):\n",
    "    test_loader = DataLoader(test_set, batch_size=64, shuffle=False)\n",
    "\n",
    "    y_test_pred = []\n",
    "    y_test_true = []\n",
    "\n",
    "    model.eval()\n",
    "    model.to('cpu')\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            try:\n",
    "                X, y = batch\n",
    "                X = X.to('cpu')\n",
    "                logits = model(X)\n",
    "            except ValueError:\n",
    "                X_temporal, X_static, y = batch\n",
    "                X_temporal, X_static = X_temporal.to('cpu'), X_static.to('cpu')\n",
    "                logits = model(X_temporal, X_static)\n",
    "\n",
    "            y_test_true.extend(y.squeeze().cpu().tolist())\n",
    "            y_test_pred.extend(logits.squeeze().cpu().tolist())\n",
    "\n",
    "    return y_test_true, y_test_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9667af58",
   "metadata": {},
   "outputs": [],
   "source": [
    "ARTIFACTS_PATH = \"/home/basbenja/Facultad/TrabajoFinal/mountpoint/mlartifacts\"\n",
    "RUNS_PATH = \"/home/basbenja/Facultad/TrabajoFinal/mountpoint/mlruns\"\n",
    "\n",
    "EXPERIMENT_NAME = f\"{EXPERIMENT_PREFIX}-{GROUP}-Comp{COMPARISON}\"\n",
    "\n",
    "experiment = mlflow.get_experiment_by_name(EXPERIMENT_NAME)\n",
    "experiment_id = experiment.experiment_id\n",
    "experiment_artifacts_path = os.path.join(ARTIFACTS_PATH, experiment_id)\n",
    "experiment_runs_path = os.path.join(RUNS_PATH, experiment_id)\n",
    "\n",
    "runs = mlflow.search_runs(\n",
    "    experiment_ids=[experiment.experiment_id],\n",
    "    output_format=\"list\",\n",
    "    filter_string=f\"params.model_arch = '{MODEL_ARCH}'\",\n",
    ")\n",
    "\n",
    "print(EXPERIMENT_NAME, experiment_id)\n",
    "print(len(runs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c439bbc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "equal_reports = {}\n",
    "best_thresholds = {}\n",
    "\n",
    "for i, run in enumerate(runs):\n",
    "    run_info = run.info\n",
    "    run_id = run_info.run_id\n",
    "    run_name = run_info.run_name\n",
    "    run_params = run.data.params\n",
    "    run_artifact_uri = run.info.artifact_uri\n",
    "\n",
    "    run_artifacts_path = os.path.join(experiment_artifacts_path, run_id, 'artifacts')\n",
    "    run_params_path = os.path.join(experiment_runs_path, run_id, 'params')\n",
    "\n",
    "    print(f\"Starting for run: {run_name}. {i+1}/{len(runs)}\")\n",
    "\n",
    "    simulation = run_params['simulation']\n",
    "    stata_filepath = os.path.join(GROUP_DIR, simulation + \".dta\")\n",
    "    df = pd.read_stata(stata_filepath)\n",
    "\n",
    "    print(\"    Building datasets\")\n",
    "    type1_df, type2_df, type3_df = get_dfs(df, REQ_PERIODS)\n",
    "\n",
    "    type3_train_ids_logged = mlflow.artifacts.load_dict(run_artifact_uri + \"/ninis_ids_train.json\")['ninis_ids_train']\n",
    "    type3_test_ids_logged  = mlflow.artifacts.load_dict(run_artifact_uri + \"/ninis_ids_test.json\")['ninis_ids_test']\n",
    "\n",
    "    type1_ids = type1_df.index.unique()\n",
    "    n_type1_train = 1000\n",
    "    type1_train_ids = np.random.choice(type1_ids, n_type1_train, replace=False)\n",
    "    type1_train_df = type1_df.loc[type1_train_ids]\n",
    "\n",
    "    type3_train_df = type3_df.loc[type3_train_ids_logged]\n",
    "    type3_test_df  = type3_df.loc[type3_test_ids_logged]\n",
    "\n",
    "    train_df = pd.concat([type1_train_df, type3_train_df])\n",
    "    X_train_df, y_train_df = train_df[FEATS], train_df['target']\n",
    "\n",
    "    test_df = pd.concat([type2_df, type3_test_df])\n",
    "    X_test_df, y_test_df = test_df[FEATS], test_df['target']\n",
    "\n",
    "    data_preprocessor = DataPreprocessor(STAT_FEATS, TEMP_FEATS)\n",
    "    train_set, test_set = data_preprocessor.build_datasets(\n",
    "        X_train_df, X_test_df, y_train_df, y_test_df, MODEL_ARCH\n",
    "    )\n",
    "\n",
    "    print(\"    Loading model\")\n",
    "    model_uri = f\"runs:/{run_id}/trained_model\"\n",
    "    model = mlflow.pytorch.load_model(model_uri)\n",
    "\n",
    "    print(\"    Evaluating model\")\n",
    "    y_test_true, y_test_pred = evaluate_model(model, test_set)\n",
    "\n",
    "    y_test_pred = np.array(y_test_pred)\n",
    "    y_test_pred_prob = 1 / (1 + np.exp(-y_test_pred))\n",
    "    y_test_pred_class = (y_test_pred_prob >= 0.5).astype(int)\n",
    "\n",
    "    print(\"    Getting logged classification report\")\n",
    "    report_logged = mlflow.artifacts.load_dict(run_artifact_uri + \"/classification_report.json\")\n",
    "    report_dict = classification_report(y_test_true, y_test_pred_class, output_dict=True)\n",
    "\n",
    "    if report_logged != report_dict:\n",
    "        print(f\"    Run {run_name} has a different classification report than logged.\")\n",
    "        print(\"    Logged report:\")\n",
    "        print(json.dumps(report_logged, indent=4))\n",
    "        print(\"    Computed report:\")\n",
    "        print(json.dumps(report_dict, indent=4))\n",
    "        break\n",
    "\n",
    "    fpr, tpr, thresholds, area, fig, ax = roc_curve_plot(y_test_true, y_test_pred_prob)\n",
    "    roc_auc_score_path = os.path.join(run_params_path, 'roc_auc_score')\n",
    "    with open(roc_auc_score_path, 'w') as f:\n",
    "        f.write(str(area))\n",
    "\n",
    "    roc_auc_plot_path = os.path.join(run_artifacts_path, 'roc_curve_plot.png')\n",
    "    fig.savefig(roc_auc_plot_path)\n",
    "    plt.close(fig)\n",
    "\n",
    "    best_thresh, best_f1 = get_best_threshold_for_f1(y_test_true, y_test_pred_prob)\n",
    "    print(f\"    Best threshold: {best_thresh:.2f} | F1 score: {best_f1:.4f} | Real F1 Score: {report_dict['1.0']['f1-score']:.4f}\")\n",
    "    equal_reports[run_name] = (report_logged == report_dict)\n",
    "    best_thresholds[run_name] = {\n",
    "        'best_threshold': best_thresh,\n",
    "        'best_f1': best_f1,\n",
    "        'real_f1': report_dict['1.0']['f1-score']\n",
    "    }\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
