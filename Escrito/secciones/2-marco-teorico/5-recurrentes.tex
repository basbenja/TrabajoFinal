\documentclass[../../main.tex]{subfiles}
% \graphicspath{{\subfix{../images/}}}

\begin{document}
Las Redes Neuronales Recurrentes (RNRs) son una familia de redes neuronales pensadas
específicamente para trabajar con \textbf{datos secuenciales}, en donde el orden y la
dinámica importan. Es por esto que han sido y continúan siendo muy útiles en tareas como
análisis de series temporales y procesamiento de lenguaje natural. Se emplean por ejemplo
para predecir cuáles van a ser los próximos valores en una serie de tiempo (tarea
comúnmente conocida como \textit{forecasting}), cuáles van a ser las palabras que
continúan una oración, y también para clasificar series de tiempo, que es para lo que las
usaremos nosotros.

Una características de las redes que hemos presentado hasta el momento es que no tienen
memoria. Si quisieramos procesar una secuencia temporal de datos con estas redes, lo que
deberíamos hacer es ``mostrársela'' entera como entrada, perdiendo la dependencia temporal
que existe entre las distintas features, que en realidad son los pasos de tiempo de una
misma secuencia.

Las RNRs proponen una forma alternativa de trabajar con datos de este tipo. Procesan las
secuencias iterando sobre los distintos pasos de tiempo manteniendo un \textbf{estado
interno} o \textbf{memoria} que contiene información relacionada con lo que ha visto hasta
el momento\footnotemark. A esto lo logran introduciendo cíclos en su grafo, que permiten
que las neuronas de la red puedan tomar como entradas sus propias salidas de pasos
anteriores. Esto provoca que entradas recibidas en pasos más tempranos afecten la
respuesta de la red ante la entrada actual \cite{ai-a-modern-approach} \footnotetext{Cabe
aclarar que este estado interno se reinicia entre el procesamiento de diferentes
secuencias.}.

Para comprender un poco más este comportamiento, tomemos la más simple de las RNRs,
compuesta por una única neurona (oculta) que recibe la entrada correspondiente al tiempo
\(t\), produce una salida y se la envía tanto a la capa de salida como a sí misma
\cite{hands-on-ML-sklearn-tf}, como se puede ver a la izquierda de la Figura
\ref{fig:simple-rnn}. Así, en cada paso \(t\), la \textit{neurona ``recurrente''} \(f\)
recibe no solo la entrada \(x_t\) sino también su propia salida computada en el paso
anterior, \(f_{t-1}\). Esto se hace aún más evidente cuando ``desenrollamos'' la red a lo
largo del tiempo, cuya representación se encuentra a la derecha de la Figura
\ref{fig:simple-rnn} y se asemeja a la de una feedforward.

% Ponerlo acá?
Otro detalle a tener en cuenta en estas redes es que, como se puede ver en la Figura
\ref{fig:simple-rnn}, en cada paso de tiempo de una misma secuencia, la red genera una
salida. Es decir, si tenemos una secuencia de largo \(T\) y nuestra red produce en cada
paso de tiempo una salida de tamaño \(o\), entonces al terminar de procesar una secuencia
entera, tendremos una salida ``total'' de tamaño \(T \times o\). Sin embargo,
la salida en cada paso de tiempo contiene información de los pasos de tiempo previos,
por lo que en general se suele utilizar la salida producida en el último paso, ya
que contiene información sobre toda la secuencia.

% Dónde agregarlo?
% Y además, permite procesar secuencias de longitud variable; a diferencia de las
% feedforward cuyo tamaño de entrada es fijo.

\begin{figure}[ht]
    \centering
    \begin{tikzpicture}[
        item/.style={circle,draw,thick,align=center},
        itemc/.style={node hidden,on chain,join}
    ]
    % Unenrolled RNN (on the right)
        \begin{scope}[
            start chain=going right,
            nodes=itemc,
            every join/.style={-latex,very thick},
            node distance=3em,
            local bounding box=chain
        ]
        \path
            node (f0) {\(f\)}
            node (f1) {\(f\)}
            node (f2) {\(f\)}
            node[xshift=2em] (ft) {\(f\)};
        \end{scope}
        \foreach \i/\j in {0/1, 1/2, 2/t} {
            \draw[very thick,-latex]
                (f\i) -- node[pos=0.4,fill=white] {\scriptsize \(w_{f,f}\)} (f\j);
        }

        \foreach \X in {0,1,2,t}{
            \draw[very thick,-latex] (f\X.north) -- ++ (0, 3.5em)
                node[node out,above,item] (y\X) {\(y_\X\)}
                node[pos=0.4,fill=white] {\footnotesize\shortstack{\(w_{0_f, y}\)\\\(w_{f, y}\)}};
            \draw[very thick,latex-] (f\X.south) -- ++ (0, -3.5em)
                node[node in,below,item] (x\X) {\(x_\X\)}
                node[pos=0.6,fill=white] {\footnotesize\shortstack{\(w_{0_x, f}\)\\\(w_{x, f}\)}};
        }
        \path (x2) -- (xt) node[midway,scale=2,font=\bfseries] {\dots};

        % Equal sign
        \node[left=1em of chain,scale=2] (eq) {\(=\)};

        % Folded RNN (on the left)
        \node[node hidden,left=2em of eq] (f) {\(f\)};
        \path (f) edge[
            out=135, in=225, looseness=5, ->, very thick
        ] node[pos=0.5,fill=white] {\scriptsize \(w_{f,f}\)} (f);
        \draw[very thick,-latex]
            (f.north) -- ++ (0, 3.5em)
            node[node out,above,item] {\(y_t\)}
            node[pos=0.4,fill=white] {\footnotesize\shortstack{\(w_{0_f,y}\)\\\(w_{f, y}\)}};
        \draw[very thick,latex-]
            (f.south) -- ++ (0, -3.5em)
            node[node in,below,item] {\(x_t\)}
            node[pos=0.6,fill=white] {\footnotesize\shortstack{\(w_{0_x,f}\)\\\(w_{x, f}\)}};
    \end{tikzpicture}
    \caption{A la izquierda, se ve la más simple de las RNRs, con solamente la capa de
    entrada, una capa oculta formada por una neurona con su respectiva conexión
    recurrente, y la capa de salida. A la derecha, se ve la misma red pero
    ``desenrollada'' a lo largo del tiempo. Asumiremos por simplicidad que tanto la
    entrada como la salida en cada momento \(t\) son simplemente números reales, es decir
    \(x_t, y_t \in \mathbb{R} \ \forall t\). Denotamos con \(w_{0_x,f}\) y \(w_{x,f}\) al
    bias y al peso respectivamente que van desde la entrada a la neurona oculta, con
    \(w_{0_f,y}\) y \(w_{f,y}\) a los que van desde la neurona oculta hasta la capa de
    salida, y con \(w_{f,f}\) al peso que aplica la neurona sobre su propia salida del
    paso anterior.}
    \label{fig:simple-rnn}
\end{figure}

Cada neurona realiza lo mismo que en una red feedforward, en el sentido que computan una
suma pesada de sus entradas y aplican una función de activación sobre esta. Sin embargo, a
partir de la Figura \ref{fig:simple-rnn}, podemos notar algunas particularidades con
respecto a las redes recurrentes:
\begin{itemize}
    \item Por un lado, cada neurona tiene un peso extra además del que se aplica sobre la
    entrada \(x_t\) (\(w_{x,f}\)) y el bias correspondiente (\(w_{0_x,f}\)). Este peso
    extra corresponde al que se aplica sobre la salida de la neurona en el paso anterior
    \(f_{t-1}\), denotado con \(w_{f,f}\).
    \item Por otro lado, la red utiliza los mismos pesos \(w_{x,f}\), \(w_{f,f}\) y
    \(w_{f,y}\), y biases \(w_{0_x,f}\) y \(w_{0_f, y}\) en \textit{todos} los pasos de
    tiempo.
\end{itemize}

Con esto en cuenta, vamos a tener que el cómputo llevado a cabo por la red
en cada paso de tiempo \(t\) está dado por:
\begin{align}
    f_t &=\ a_f \left( w_{x,f} x_t  + w_{f,f} f_{t-1} + w_{0_x,f} \right) \label{eq:ft} \\
    y_t &=\ a_y \left( w_{f,y} f_t + w_{0_f, y} \right) \label{eq:yt}
\end{align}
donde \(a_f\) denota la función de activación de la capa oculta y \(a_y\) la de la capa de
salida. Algo que vale la pena aclarar es que para la entrada \(x_0\), correspondiente al
primer paso de tiempo, lo que sería la salida del paso anterior se establace manualmente,
por lo general con un valor nulo.

Nos concentremos en la salida de la neurona recurrente en el tiempo \(t\), es decir
en \(f_t\), dejando la salida de la red \(y_t\) de lado. A partir de las Ecuaciones
anteriores, podemos ver lo que venimos explicando: \(f_t\) es una función de
tanto la entrada en el tiempo actual \(x_t\) y de su salida en el paso anterior
\(f_{t-1}\). Entonces, si tomamos un \(t' > 0\) fijo, vamos a tener que:
\begin{itemize}[itemsep=0.05cm]
    \item \(f_{t'}\) es una función de \(x_{t'}\) y de \(f_{t'-1}\), pero
    \item \(f_{t'-1}\) es a su vez una función de \(x_{t'-1}\) y de \(f_{t'-2}\), pero
    \item \(f_{t'-2}\) es a su vez una función de \(x_{t'-2}\) y de \(f_{t'-3}\),
    \item y así sucesivamente.
\end{itemize}
Este comportamiento hace que \(f_{t'}\) sea una función de todas las entradas vistas desde
\(t=0\), constituyendo de esta forma una especie de \textit{memoria}, que se suele llamar
\textbf{estado oculto} de la neurona. En este caso, como la red solamente tiene una capa
oculta con una neurona recurrente, el estado oculto de la neurona coincide con el estado
oculto de la red.

De la misma forma en que lo hicimos anteriormente para las redes feedforward, podemos
empezar a complejizar esta red agregando varias neuronas recurrentes en la capa oculta,
cada una con su propio ciclo, como se puede ver en la Figura \ref{fig:simple-rnn-2}.
\begin{figure}[ht]
    \centering
    \begin{tikzpicture}[
        item/.style={circle, draw, thick, align=center, minimum size=2em},
        >=latex
    ]
    \def\n{5}    % number of recurrent neurons
    \def\xsep{3} % horizontal separation in em

    \node[node in] (x) at ({(\n-1)*\xsep/2}, -3.5) {\(x_t\)};
    \node[node out] (y) at ({(\n-1)*\xsep/2}, 3.5) {\(y_t\)};

    % Loop through to create f_i nodes
    \foreach \i in {1,...,\n} {
        \pgfmathsetmacro{\x}{(\i-1)*\xsep}
        \node[node hidden] (f\i) at (\x, 0) {\(f_{\i}\)};

        % Arrow up to y_t
        \draw[very thick,-latex]
            (f\i.north) -- (y)
            node[pos=0.4,fill=white] {\footnotesize{\(w_{f_\i, y}\)}};

        % Self-loop
        \path (f\i) edge[out=135, in=225, looseness=6, ->, very thick]
            node[pos=0.5,fill=white] {\scriptsize \(w_{f_\i,f_\i}\)} (f\i);
        % Arrow down from x_t
        \draw[very thick,latex-]
            (f\i.south) -- (x)
            node[pos=0.4,fill=white] {\footnotesize{\(w_{x, f_\i}\)}};
    }
    \end{tikzpicture}
    \caption{RNR con una capa oculta de 5 neuronas recurrentes, cada una con su ciclo.
    Similarmente a la Figura anterior, denotamos con \(w_{x,f_i}\) al peso que va desde la
    capa de entrada a la neurona oculta \(f_i\), con \(w_{f_i, f_i}\) al peso de cada
    neurona oculta a sí misma, y con \(w_{f_i, y}\) al que va desde la neurona oculta
    \(f_i\) a la capa de salida (\(i=1,2,3,4,5\)). En este caso, omitimos los biases.}
    \label{fig:simple-rnn-2}
\end{figure}

Con esto, vamos a tener que el estado oculto de la red va a ser un vector formado por el
estado oculto de las 5 neuronas. O sea, si denotamos con \(f_t\) al estado oculto
de la red en el tiempo \(t\) y con \(f_{i_t}\) al estado oculto de la neurona oculta
\(i\), en el tiempo \(t\) vamos a tener que:
\begin{equation*}
    f_t = (f_{1_t}, f_{2_t}, f_{3_t}, f_{4_t}, f_{5_t})
\end{equation*}

Y también podemos volver a utilizar la notación matricial para describir el comportamiento
de la red de manera general:
\begin{align*}
    f_t &=\ \bm{a}_f \left( \mathbf{W}_x x^T + \mathbf{W}_f f_{t-1}^T \right) \\
    y_t &=\ \bm{a}_y \left( \mathbf{W}_y f_t \right)
\end{align*}
donde, siendo \(k\) el número de neuronas en la capa oculta y \(n\) el tamaño de cada
paso de tiempo de la secuencia:
\begin{itemize}[itemsep=0.05cm]
    \item \(\bm{a}_f\) es la función de activación de la capa oculta, aplicada elemento a
    elemento, es decir \(\bm{a}_f(z_0, z_1, ..., z_m) = (a_f(z_0), a_f(z_1), ...,
    a_f(z_m))\).
    \item \(\mathbf{W}_x\) es la matriz de tamaño \(k \times (n+1)\) de pesos que salen
    desde la capa de entrada hasta la capa oculta, incluyendo el bias.
    \item \(x^T\) es el vector transpuesto de la entrada, junto con el 1 del bias,
    es decir \(x^T\) es de tamaño \((n+1) \times 1\), y corresponde a \textit{un}
    paso de tiempo.
    \item \(\mathbf{W}_f\) es la matriz diagonal de tamaño \(k \times k\) de pesos
    recurrentes. En cada elemento de la diagonal \(i\) se encuentra el peso que va desde
    la neurona \(i\) a sí misma, es decir \(w_{f_i, f_i}\).
    \item \(f_{t}^T\) es el vector transpuesto del estado oculto de la red en el paso de
    tiempo \(t\), es decir es de tamaño \(k \times 1\). Al agregar el 1 que acompaña al
    bias que se dirige a la capa de salida, termina quedando de tamaño \((k+1) \times 1\)
    \item \(\bm{a}_y\) es la función de activación de la capa de salida, aplicada elemento a
    elemento, al igual que \(\bm{a}_t\).
    \item \(\mathbf{W}_y\) es la matriz de tamaño \(o \times (k+1)\) de pesos que van
    desde la capa oculta hasta la capa de salida, incluyendo el bias, donde \(o\) es el
    tamaño de la salida.
\end{itemize}

Al igual que antes, se pueden agregar más capas de neuronas recurrentes a la red,
permitiendo de esta manera que cada capa tenga su propio estado oculto. En este caso, lo
que va a ocurrir es que el estado oculto producido por las neuronas de una capa va a ser
la entrada de las neuronas de la siguiente capa, estando las capas totalmente conectadas
entre sí.

Para entrenar este tipo de redes, el truco está en ``desenrollarlas'' como vimos
anteriormente y luego utilizar el algoritmo de backpropagation presentado en la sección de
Redes Neuronales. Esta estrategia es conocida como \textbf{\textit{backpropagation through
time}}.

Ahora bien, estas redes en su forma estándar sufren de un problema conocido como
\textbf{el problema del gradiente desvaneciente}, que las hace incapaces de capturar
dependencias de largo plazo al procesar secuencias de muchos pasos de tiempo.

En general, y no solo en estas, el problema del gradiente desvaneciente ocurre cuando, al
calcular el gradiente de la función de pérdida con respecto a un parámetro determinado,
este resulta ser muy pequeño. Como consecuencia, la actualización de dicho parámetro
durante el proceso de entrenamiento es insignificante, lo que puede provocar que la red
aprenda muy lentamente o incluso que deje de entrenarse por completo.

\subsection{Long Short-Term Memory}
Las redes conocidas como \textit{Long Short-Term Memory} (LSTM) son un tipo particular de
RNRs que fueron introducidas en el año 1997 \cite{lstm-paper-1997} para solucionar el
problema del gradiente desvaneciente que tenían las RNRs estándar, haciéndolas capaces
de aprender dependencias de largo plazo. Han demostrado funcionar de manera efectiva en
una gran variedad de problemas, y los mayores avances obtenidos con redes recurrentes se
han logrado utilizando esta arquitectura \cite{colahs-blog-lstm-2015}.

Como hemos visto hasta el momento, todas las redes recurrentes tienen una estructura de
cadena de ``módulos'' o ``bloques'' \cite{colahs-blog-lstm-2015} - las neuronas
recurrentes -, es decir, unidades que se repiten a lo largo de la secuencia temporal. En
una RNR estándar, este módulo es simplemente una aplicación de una función de activación,
particularmente la tangente hiperbólica, sobre una suma pesada de sus entradas. En las
LSTM, estas neuronas se complejizan para incorporar mecanismos que permiten mantener
una memoria a largo plazo en la red. Las llamaremos ``celdas'' o neuronas LSTM.

El principal componente que introducen las LSTM para poder retener información por largos
períodos de tiempo es el \textbf{estado de celda}. La particularidad de este estado es que
a medida que ``fluye'' por la red, se mantiene prácticamente intacto, salvo por algunas
interacciones lineales \cite{colahs-blog-lstm-2015}, pero no es afectado por ningún peso o
bias, lo que permite justamente solucionar el desvanecimiento del gradiente.

Así, vamos a tener dos estados que interactúan entre sí para realizar predicciones:
\begin{itemize}
    \item Por un lado, el \textbf{estado de celda}, que representa la \textbf{memoria a largo
    plazo} de la red, y no es afectado por ningún peso o bias.
    \item Por otro lado, el \textbf{estado oculto} (del que hablamos previamente), que
    representa la \textbf{memoria a corto plazo} de la red, y sí es calculado por medio
    de sumas pesadas.
\end{itemize}

Ahora bien, con esto, la idea es que la red aprenda qué almacenar en el estado de celda,
qué descartar y qué leer de él \cite{hands-on-ML-sklearn-tf}, y a esto lo hace a través de
otros nuevos elementos llamados \textbf{compuertas}. En particular, se introducen tres
tipos de compuertas en la celda LSTM:
\begin{itemize}
    \item \textbf{La compuerta de olvido \(\bm{f}\)} (del inglés \textit{forget}):
    determina si cada elemento del estado de celda es recordado, copiándolo al paso
    siguiente, u olvidado, fijándolo en 0.
    \item \textbf{La compuerta de entrada \(\bm{i}\)} (del inglés \textit{input}):
    determina cuál va a ser la información que se va a utilizar para actualizar
    el estado de celda a partir de la entrada actual y del estado oculto actual.
    \item \textbf{La compuerta de salida \(\bm{o}\)} (del inglés \textit{output})
    determina qué partes de la memoria de largo plazo son transferidas a la memoria
    de corto plazo, el estado oculto.
\end{itemize}

De esta forma, las compuertas son una manera de regular el paso de información
\cite{colahs-blog-lstm-2015}. La salida de cada compuerta es un vector de valores, todos en
el rango \([0,1]\). Estos se obtienen como la salida de una pequeña red neuronal
totalmente conectada en la que las entradas son los datos del paso de tiempo actual y el
estado oculto anterior, y cuya capa de salida aplica una función de activación sigmoide.

A continuación, detallamos las operaciones producidas por una celda LSTM en un paso de
tiempo. Utilizaremos la siguiente notación para un tiempo \(t\):
\begin{itemize}[itemsep=0.1cm]
    \item \(x_t\) para el vector de entrada.
    \item \(h_t\) para el estado oculto.
    \item \(C_t\) para el estado de celda.
    \item \(\tilde{C}_t\) para el ``potencial'' estado de celda (veremos a continuación
    qué significa esto).
    \item \(f_t\), \(i_t\), \(o_t\) para las salidas de las compuertas de olvido,
    entrada y salida respectivamente.
\end{itemize}
Cabe aclarar que \(h_t\), \(C_t\), \(\tilde{C}_t\), \(f_t\), \(i_t\) y \(o_t\) son todos
vectores de la misma dimensión. Con estos elementos, podemos dar un diagrama de una celda
LSTM, que se encuentra en la Figura \ref{fig:lstm-cell}.

\begin{figure}
    % Fuente: https://tex.stackexchange.com/a/432344
    \centering
    \begin{tikzpicture}[
        % GLOBAL CFG
        font=\sf, >=LaTeX,
        % Styles
        cell/.style={ % For the main box
            rectangle,
            rounded corners=5mm,
            draw,
            very thick,
            fill=myblue!20
        },
        operator/.style={ % For operators like +  and  x
            circle,
            draw,
            inner sep=-0.5pt,
            minimum height =.2cm,
            fill=myorange!50!white
        },
        function/.style={ % For functions
            ellipse,
            draw,
            inner sep=1pt,
            fill=myyellow
        },
        ct/.style={ % For external inputs and outputs
            circle,
            draw,
            line width = .75pt,
            minimum width=1cm,
            inner sep=1pt,
        },
        gt/.style={ % For internal inputs
            rectangle,
            draw,
            minimum width=4mm,
            minimum height=3mm,
            inner sep=1pt,
            fill=myyellow
        },
        ArrowC1/.style={ % Arrows with rounded corners
            rounded corners=.25cm,
            thick,
        },
        ArrowC2/.style={ % Arrows with big rounded corners
            rounded corners=.5cm,
            thick,
        },
        ]

        % Draw the cell:
        \node [cell, minimum height=4cm, minimum width=6cm] at (0,0) {} ;

        % Draw inputs named ibox#
        \node [gt] (ibox1) at (-2,0) {\(\sigma\)};
        \node [gt] (ibox2) at (-1.5, -1) {\(\sigma\)};
        \node [gt, minimum width=1cm] (ibox3) at (-0.5, -1) {tanh};
        \node [gt] (ibox4) at (0.5, -1) {\(\sigma\)};

        % Draw operators   named mux# , add# and func#
        \node [operator] (mux1) at (-2,1.5) {\(\odot\)};
        \node [operator] (add1) at (-0.5,1.5) {+};
        \node [operator] (mux2) at (-0.5,0) {\(\odot\)};
        \node [operator] (mux3) at (1.5,0) {\(\odot\)};
        \node [function] (func1) at (1.5,0.75) {tanh};

        % Draw External inputs? named as basis c,h,x
        \node[ct] (c) at (-4,1.5) {\(C_{t-1}\)};
        \node[ct] (h) at (-4,-1.5) {\(h_{t-1}\)};
        \node[ct, fill=mygreen!25] (x) at (-2.5,-3) {\(x_t\)};

        % Draw External outputs? named as basis c2,h2,x2
        \node[ct] (c2) at (4,1.5) {\(C_t\)};
        \node[ct] (h2) at (4,-1.5) {\(h_t\)};
        \node[ct] (x2) at (2.5,3) {\(h_t\)};

        % Start connecting all.
        % Intersections and displacements are used.
        % Drawing arrows
        \draw [ArrowC1] (c) -- (mux1) -- (add1) -- (c2);

        % Inputs
        \draw [ArrowC1] (h) -| (ibox4);
        \draw [ArrowC1] (h -| ibox1)++(-0.5,0) -| (ibox1);
        \draw [ArrowC1] (h -| ibox2)++(-0.5,0) -| (ibox2);
        \draw [ArrowC1] (h -| ibox3)++(-0.5,0) -| (ibox3);
        \draw [ArrowC1] (x) -- (x |- h)-| (ibox3);

        % Internal
        \draw [->, ArrowC2] (ibox1) -- (mux1) node[midway, left] {\scriptsize{\(f_t\)}};
        \draw [->, ArrowC2] (ibox2) |- (mux2) node[midway] {\scriptsize{\(i_t\)}};
        \draw [->, ArrowC2] (ibox3) -- (mux2) node[midway, right] {\scriptsize{\(\tilde{C}_t\)}};
        \draw [->, ArrowC2] (ibox4) |- (mux3) node[midway] {\scriptsize{\(o_t\)}};
        \draw [->, ArrowC2] (mux2) -- (add1);
        \draw [->, ArrowC1] (add1 -| func1)++(-0.5,0) -| (func1);
        \draw [->, ArrowC2] (func1) -- (mux3);

        %Outputs
        \draw [-, ArrowC2] (mux3) |- (h2);
        \draw (c2 -| x2) ++(0,-0.1) coordinate (i1);
        \draw [-, ArrowC2] (h2 -| x2)++(-0.5,0) -| (i1);
        \draw [-, ArrowC2] (i1)++(0,0.2) -- (x2);
    \end{tikzpicture}
    \caption{Diagrama de una celda LSTM. \(\sigma\) representa la función sigmoide, tanh
    la tangente hiperbólica, el signo ``+'' una suma, y el signo \(\odot\) el producto de
    Hadamard, también conocido como ``multiplicación elemento a elemento''.}
    \label{fig:lstm-cell}
\end{figure}

Y ahora, como se puede deducir a partir de la Figura \ref{fig:lstm-cell}, vamos a tener
diversas matrices de pesos involucradas, que todas se aprenden durante el entrenamiento.
Dejando de lado sus dimensiones, y asumiendo que todas incluyen el bias, las denotaremos
de la siguiente forma:
\begin{itemize}
    \item \(\bm{W}_{x,f}\) y \(\bm{W}_{h,f}\) para los pesos de la compuerta de olvido.
    \item \(\bm{W}_{x,i}\) y \(\bm{W}_{h,i}\) para los pesos de la compuerta de entrada.
    \item \(\bm{W}_{x,o}\) y \(\bm{W}_{h,o}\) para los pesos de la compuerta de salida.
    \item \(\bm{W}_{x, \tilde{C}}\) y \(\bm{W}_{h, \tilde{C}}\) para los pesos de la potencial
    memoria a largo plazo.
\end{itemize}
En cada una, los subíndices indican de dónde hacia dónde van los pesos. Por ejemplo,
\(\bm{W}_{x,f}\) es la matriz de pesos que va desde la entrada \(x_t\) hacia la compuerta
de olvido \(f_t\), y \(\bm{W}_{h,f}\) es la matriz de pesos que va desde el estado oculto
\(h_{t-1}\) hacia la compuerta de olvido \(f_t\). Con esto, veamos exactamente qué
hace una neurona LSTM.

Para empezar, la celda toma tres entradas: el estado de celda del paso anterior
\(C_{t-1}\), el estado oculto del paso anterior \(h_{t-1}\), y el vector de entrada del
paso actual \(x_t\). En cada paso, las tres compuertas actúan ``internamente'' para
producir dos salidas: el nuevo estado de celda \(C_t\) y el nuevo estado oculto \(h_t\).

La primera compuerta que actúa es la de olvido, para determinar que información ya
presente en el estado de celda se ``sigue recordando''. Esta decisión se toma aplicando
una función sigmoide sobre el estado oculto anterior \(h_{t-1}\), y la entrada actual
\(x_t\), produciendo un número entre 0 y 1 para cada valor presente en \(C_{t-1}\):
\[
    f_t = \sigma \left( \bm{W}_{x,f} x_t + \bm{W}_{x,h} h_{t-1}  \right)
\]

El próximo paso es decidir qué nueva información ingresa a \(C_t\) utilizando la compuerta
de entrada. Esta etapa se puede ver como compuesta por dos partes:
\begin{itemize}
    \item Por un lado, se calcula una potencial memoria a largo plazo \(\tilde{C}_t\).
    Esto se hace combinando el estado oculto del paso anterior \(h_{t-1}\) y la entrada
    actual \(x_t\), y aplicando una tangente hiperbólica:
    \[
        \tilde{C}_t = \text{tanh} \left( \bm{W}_{x, \tilde{C}} x_t + \bm{W}_{h, \tilde{C}} h_{t-1}  \right)
    \]
    \item Por otro lado, a través de la compuerta de entrada, se calcula cuáles
    van a ser los valores de la memoria a largo plazo que se van a actualizar:
    \[
        i_t = \sigma \left( \bm{W}_{x, i} x_t + \bm{W}_{h, i} h_{t-1}  \right)
    \]
\end{itemize}

Con estas tres cantidades (\(f_t\), \(\tilde{C}_t\), \(i_t\)), es momento de
efectivamente actualizar el estado de celda:
\begin{enumerate}
    \item En primer lugar, se multiplica elemento a elemento \(C_{t-1}\) por \(f_t\). Esto
    provoca que un valor cercano a 0 en \(f_t\) haga que el valor correspondiene en
    \(C_{t-1}\) se ``olvide'', y que un valor cercano a 1 haga que se siga recordando.
    \item Y luego, se le suma la nueva información, dada por la multiplicación
    elemento a elemento entre \(\tilde{C}_t\) e \(i_t\), que intuitivamente representa qué
    valores se van a actualizar y en qué medida.
\end{enumerate}
De esta forma, la nueva memoria a largo plazo está dada por:
\[
    C_t = C_{t-1} \odot f_t + \tilde{C}_t \odot i_t
\]

Por último, es necesario determinar cuál va a ser el nuevo estado oculto de la celda.
Este se construye como una versión ``filtrada'' del nuevo estado de celda, haciendo
uso del valor de la compuerta de salida:
\[
    o_t = \sigma \left( \bm{W}_{x,o} x_t + \bm{W}_{h,o} h_{t-1} \right)
\]
que, combinado con una aplicación de tanh sobre el nuevo estado de celda para situar a
todos los valores en el rango \([-1,1]\), da la nueva memoria a corto plazo:
\[
    h_t = o_t \odot \text{tanh}(C_t)
\]

\bigskip
Una aclaración es que cuando estas redes se utilizan para llevar a cabo clasificación
de secuencias, lo que se hace es tomar el estado \textbf{oculto} del último paso y,
similarmente a lo que se hace en las redes convolucionales, se agregan una o más capas
totalmente conectadas que procesen este vector para hacer la clasificación final.

\end{document}